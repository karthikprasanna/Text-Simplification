# **Overview**
![plot](./images/overview.png
)

# **Documentation:** https://github.com/karthikprasanna/Text-Simplification/blob/main/report.pdf

# Candidate Generation 
To get the candidates using dissim we need to use the "split.py" file which would generate candidates based on dissim's model. This would again be split as required data with <$> token separating two candidates which belong to the same word and <#> separates the canidates which belong to the different words. 

To run this we just need to put our input in input.txt and just run the python file. In our code we just left it with train.src which was the actual code used for training the model.


# Candidate ranking 

We have the file GoldRanking.py which takes in the candidates generated in the previous step and assign a gold score for each candidate by considering their bert scores and their length. We then save these scores and give the saved scores file to the ChooseBestCandidate.py file which would give us the 1st ranked candidate as the output. 


# Simplification 
We finally have the training.ipynb file which fine-tuned BART based on our requirement. This file generated models which we will use to generate our final output. Thefinal output is generated by model.py which firstly loads the trained models and then predicts the output based on the input given. This outputs are finally given to the metrics with the test_output to get the metrics of our model.

# Generating metrics 

To generate metrix we first need to put our data into input.txt and output.txt. Now we just give the input and output files to "CalculateBLEUscore.py","CalculateSARIscore.py","CalculateBERTscore.py" to get the quantitave measure of the model. 

# Dataset
We have used the ACL2020 dataset which can be found in the link: https://iiitaphyd-my.sharepoint.com/:f:/g/personal/prasanna_karthik_research_iiit_ac_in/Eg1G3Mu1E9JLqyVHoC3sn5YBUbLqzOL43UW5Eta79DjO8w?e=q0tUd8


# Hyperparameters Used
- batch_size = 8
- lr = default
- epochs = 1
- max_length = default
- tokenizer = facebook/bart-large-cnn

# Models

- The link of our primitive model with copy ratio: https://iiitaphyd-my.sharepoint.com/:f:/g/personal/prasanna_karthik_research_iiit_ac_in/EjSUoWyGVU1Dn6MFnYCxjOUBNFhObOzc_vl0k1GSUmpkpw?e=71bdEq

- The link of our final model: https://iiitaphyd-my.sharepoint.com/:f:/g/personal/prasanna_karthik_research_iiit_ac_in/EhKJADOIXBNDnMX9DD8EtAsB1iIYI-utY6SS4QB3o0MNjQ?e=yiROaU
