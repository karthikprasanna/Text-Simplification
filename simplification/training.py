# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12kS9j3LbGc-EV65p38CkuVjjyvyCkSC2
"""

import string
punct = string.punctuation

def calculateCopyRatio(input1, input2):
  Dict = {}
  output = []
  input1 = input1.lower()
  input2 = input2.lower()
  input1 = input1.translate(str.maketrans('','',string.punctuation))
  input2 = input2.translate(str.maketrans('','',string.punctuation))
  i = 0

  for p in input1.split():
      j = 0
      for q in input2.split():
          if p == q:
              if j in Dict.keys():
                  j += 1
                  continue
              else:
                  Dict[j] = 1
                  output.append(1)
                  break
          j += 1
      if len(output) == i:
          output.append(0)
      i += 1

  sum = 0
  for i in range(len(output)):
      sum += output[i]
  return int(sum*100/len(output))

fileName1 = '/content/candidates.txt'
file1 = open(fileName1, 'r')
candidate = ""
candidates = []


file2 = open('/content/train.dst', 'r')
sentence_dst = ""

file3 = open('/content/generated.txt', 'w')


# read and print the first sentence in the file
def readLine(file):
    sentence = ""
    for line in file:
        for ch in line:
            if ch != '.':
                sentence += ch
            else:
                sentence += ch
                return sentence

import os
file_size = os.path.getsize(fileName1)
bytes_read = 0
def readCandidates(file):
    candidates = []
    marker = ""
    for line in file:
        for ch in line:
            global bytes_read
            global candidate
            bytes_read += 1

            if ch == '<' or ch == '#' or ch == '$' or ch == '>':
                marker += ch
            else:
                if marker == "<#>":
                    marker = ""
                    return candidates
                elif marker == "<$>":
                    marker = ""
                    candidates.append(candidate)
                    candidate = ""
                elif marker != "":
                    candidate += marker
                    marker = ""
                candidate += ch
    return candidates

def getBatch(batch_size):
    i = 0
    x = []
    y = []
    global sentence_dst, candidates
    while i < batch_size:
        if len(candidates) != 0:
            cand = candidates.pop()
            cand = f'{calculateCopyRatio(cand, sentence_dst)} {cand}'
            x.append(cand)
            y.append(sentence_dst)
            i += 1
        else:
            sentence_dst = readLine(file2)
            candidates = readCandidates(file1)
            if sentence_dst == None or candidates == None:
                return None

    return x, y

import torch
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

!pip install transformers
from transformers import BartTokenizer, BartForConditionalGeneration

tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

model = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")

model = model.to(device)

import torch
# Create torch dataset
class Dataset(torch.utils.data.Dataset):
    def __init__(self, inputs, targets):
        self.inputs = inputs
        self.targets = targets
    
    def __len__(self):
        return len(self.targets)
    
    def __getitem__(self, index):
        input_ids = torch.tensor(self.inputs["input_ids"][index]).squeeze()
        target_ids = torch.tensor(self.targets["input_ids"][index]).squeeze()
        
        return {"input_ids": input_ids, "labels": target_ids}

# Define Trainer
import transformers
args = transformers.TrainingArguments(
    output_dir="output",
    num_train_epochs=1,

    per_device_train_batch_size=8

)

batch = getBatch(8)
print(batch)
if batch != None:
  x, y = batch
while len(x) == 8:
  x = tokenizer(x, return_tensors="np", padding=True)
  y = tokenizer(y, return_tensors="np", padding=True)

  dataset = Dataset(x, y)

  trainer = transformers.Trainer(
      model=model,
      args=args,
      train_dataset=dataset,
  )

  trainer.train()
  print(f'progress: {bytes_read*100/file_size}')
  try: 
    x, y = getBatch(8)
  except TypeError:
    break

trainer.save_model("simplifier.model")

file1.close()
file3.close()
file2.close()